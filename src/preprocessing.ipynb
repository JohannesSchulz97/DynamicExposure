{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fccaa68-5792-4217-8ec6-e8f388a1c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from tools import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78816c5-fa9c-4955-9218-4ba531492a11",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39458c6e-f44a-4000-a78e-9aab013564ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_dataset(contacts, co_locations, dataset): \n",
    "    if dataset == \"primary_school\":\n",
    "        contacts[\"time\"] = contacts[\"time\"].map(lambda x: np.round(x/600))\n",
    "        co_locations[\"time\"] = co_locations[\"time\"].map(lambda x: np.round((x-3600)/600))\n",
    "    else:  \n",
    "        contacts[\"time\"] = contacts[\"time\"].map(lambda x: np.round(x/3600))\n",
    "        co_locations[\"time\"] = co_locations[\"time\"].map(lambda x: np.round(x/3600))\n",
    "    return contacts, co_locations\n",
    "\n",
    "'''\n",
    "    assumes the columns to be in a particular form\n",
    "    - contacts and co_locations: ['time', 'from', 'to']\n",
    "    - labels: ['node', 'community']\n",
    "'''\n",
    "def relabel_dataset(contacts, co_locations, labels): \n",
    "\n",
    "    '''\n",
    "        apply node mapping\n",
    "    '''\n",
    "    node_mapping = {node: i for i,node in enumerate(labels[\"node\"].unique())}\n",
    "    contacts[[\"from\", \"to\"]] = contacts[[\"from\", \"to\"]].applymap(lambda x: node_mapping[x])\n",
    "    co_locations[[\"from\", \"to\"]] = co_locations[[\"from\", \"to\"]].applymap(lambda x: node_mapping[x])\n",
    "    labels[\"node\"] = labels[\"node\"].map(lambda x: node_mapping[x])\n",
    "\n",
    "\n",
    "    '''\n",
    "        apply label mapping \n",
    "    '''\n",
    "    label_mapping = {label: i for i,label in enumerate(labels[\"community\"].unique())}\n",
    "    labels[\"community\"] = labels[\"community\"].map(lambda x: label_mapping[x])\n",
    "    labels = labels.reset_index().drop(columns=[\"index\"])\n",
    "    return contacts, co_locations, labels\n",
    "\n",
    "def filter_dataset(contacts, co_locations, labels): \n",
    "    nodes_contacts = set(contacts[\"from\"]).union(set(contacts[\"to\"]))\n",
    "    nodes_co_locations = set(co_locations[\"from\"]).union(set(co_locations[\"to\"]))\n",
    "    nodes_labels = set(labels[\"node\"])\n",
    "    #print(nodes_contacts.difference(nodes_co_locations))\n",
    "    ''' \n",
    "        filter contacts based on nodes in labels\n",
    "    '''\n",
    "    excluded_nodes = nodes_contacts.difference(nodes_labels)\n",
    "    print(\"excluded nodes: \", excluded_nodes)\n",
    "    if len(excluded_nodes) != 0: \n",
    "        condition = np.any([((contacts[\"from\"] == node) | (contacts[\"to\"] == node)) for node in excluded_nodes], axis=0)\n",
    "        contacts = contacts.drop(contacts[condition].index)\n",
    "           \n",
    "    '''\n",
    "        filter co_locations based on nodes in contacts\n",
    "    '''\n",
    "    excluded_nodes = nodes_co_locations.difference(nodes_contacts)\n",
    "    print(\"excluded nodes: \", excluded_nodes)\n",
    "    if len(excluded_nodes) != 0: \n",
    "        condition = np.any([((co_locations[\"from\"] == node) | (co_locations[\"to\"] == node)) for node in excluded_nodes], axis=0)\n",
    "        co_locations = co_locations.drop(co_locations[condition].index)\n",
    "            \n",
    "    '''\n",
    "        filter co_locations based on times in contacts\n",
    "    '''\n",
    "    \n",
    "    excluded_times = set(co_locations[\"time\"]).difference(set(contacts[\"time\"]))\n",
    "    print(\"excluded times: \", excluded_times)\n",
    "    if len(excluded_times) != 0: \n",
    "        condition = np.any([(co_locations[\"time\"] == time) for time in excluded_times], axis=0)\n",
    "        co_locations = co_locations.drop(co_locations[condition].index)\n",
    "    \n",
    "    print(\"times in contacts but not in co_locations: \", set(contacts[\"time\"]).difference(set(co_locations[\"time\"])))\n",
    "    '''\n",
    "        filter labels based on nodes in contacts\n",
    "    '''\n",
    "\n",
    "    strangers = nodes_labels.difference(nodes_contacts)\n",
    "    if len(strangers) != 0:\n",
    "        condition = np.any([(labels[\"node\"] == stranger) for stranger in strangers], axis=0)\n",
    "        labels = labels.drop(labels[condition].index)\n",
    "        \n",
    "    nodes_contacts = set(contacts[\"from\"]).union(set(contacts[\"to\"]))\n",
    "    nodes_co_locations = set(co_locations[\"from\"]).union(set(co_locations[\"to\"]))\n",
    "    nodes_labels = set(labels[\"node\"])   \n",
    "    assert(nodes_contacts == nodes_labels)\n",
    "    #assert(nodes_contacts == nodes_co_locations), nodes_contacts.difference(nodes_co_locations)\n",
    "    #assert(nodes_labels == nodes_co_locations)\n",
    "    return contacts, co_locations, labels\n",
    "\n",
    "def create_data(contacts): \n",
    "    for t, hour in enumerate(contacts[\"time\"].unique()): \n",
    "        data_t = contacts[contacts[\"time\"] == hour]\n",
    "        dft = pd.DataFrame.from_dict(dict({\"source\": data_t[\"from\"], \"target\": data_t[\"to\"], f\"T{t}\": [1] * len(data_t.index)}))\n",
    "        dft = dft.groupby(['source', 'target']).aggregate({f'T{t}': 'sum'}).reset_index()\n",
    "        if t == 0:\n",
    "            df = dft\n",
    "        else: \n",
    "            df = df.merge(dft, on=[\"source\", \"target\"], how=\"outer\")\n",
    "\n",
    "    df = df.fillna(0)\n",
    "    df = df.astype('int32')\n",
    "    df = df.set_index([\"source\", \"target\"])\n",
    "    data = np.zeros((T, N, N))\n",
    "    for i, row in df.iterrows():\n",
    "        for t in range(T): \n",
    "            data[t,i[0],i[1]] = row[\"T\"+str(t)]\n",
    "            \n",
    "    \"\"\"\n",
    "        make the data symmetric and set diagonal to zero\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for t in range(T):\n",
    "        for i in range(N): \n",
    "            for j in range(N): \n",
    "                data[t,i,j] = max(data[t,i,j], data[t,j,i])\n",
    "                data[t,j,i] = data[t,i,j]\n",
    "        np.fill_diagonal(data[t], 0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_Z(co_locations, data): \n",
    "    T, N = data.shape[0], data.shape[1]\n",
    "    print(f\"T: {T}, N: {N}\")\n",
    "    for t, hour in enumerate(contacts[\"time\"].unique()): \n",
    "        data_t = co_locations[co_locations[\"time\"] == hour]\n",
    "        dft = pd.DataFrame.from_dict(dict({\"source\": data_t[\"from\"], \"target\": data_t[\"to\"], f\"T{t}\": [1] * len(data_t.index)}))\n",
    "        dft = dft.groupby(['source', 'target']).aggregate({f'T{t}': 'sum'}).reset_index()\n",
    "        if t == 0:\n",
    "            df = dft\n",
    "        else: \n",
    "            df = df.merge(dft, on=[\"source\", \"target\"], how=\"outer\")\n",
    "\n",
    "    df = df.fillna(0)\n",
    "    df = df.astype('int32')\n",
    "    df = df.set_index([\"source\", \"target\"])\n",
    "    \n",
    "    \n",
    "    exp_matrix = np.zeros((T, N, N))\n",
    "    for i, row in df.iterrows():\n",
    "        for t in range(T): \n",
    "            exp_matrix[t,i[0],i[1]] = row[\"T\"+str(t)]\n",
    "    '''\n",
    "        create Z for markov inference\n",
    "    '''\n",
    "    Z_markov = exp_matrix>0\n",
    "    \n",
    "    '''\n",
    "        ensure that exposure is always 1 if an interaction is observed\n",
    "    '''\n",
    "    Z_markov = np.maximum(Z_markov, data>0)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        make Z symmetric and set diagonal to zero\n",
    "    \"\"\"\n",
    "    for t in range(T):\n",
    "        for i in range(N): \n",
    "            for j in range(N): \n",
    "                Z_markov[t,i,j] = max(Z_markov[t,i,j], Z_markov[t,j,i])\n",
    "                Z_markov[t,j,i] = Z_markov[t,i,j]\n",
    "        np.fill_diagonal(Z_markov[t], 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "        create Z for heaviside inference\n",
    "    '''\n",
    "    exp_matrix = np.append(exp_matrix, np.ones((1,N,N)), axis=0)\n",
    "    Z_heaviside = np.argmax(exp_matrix>0, axis=0)\n",
    "    \n",
    "    '''\n",
    "        ensure that exposure happens before or at the same time of the first interaction\n",
    "    '''\n",
    "    interaction_tensor = np.append(data!=0, np.ones((1,N,N)), axis=0)\n",
    "    first_interaction = np.argmax(interaction_tensor, axis=0)\n",
    "    Z_heaviside = np.minimum(Z_heaviside, first_interaction)\n",
    "    \n",
    "    \"\"\"\n",
    "        make Z symmetric and set diagonal to zero\n",
    "    \"\"\"\n",
    "    for i in range(N): \n",
    "        for j in range(N): \n",
    "            Z_heaviside[i,j] = min(Z_heaviside[i,j], Z_heaviside[j,i])\n",
    "            Z_heaviside[j,i] = Z_heaviside[i,j]\n",
    "    np.fill_diagonal(Z_heaviside, 0)\n",
    "    \n",
    "    return Z_heaviside, Z_markov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fad786-354a-478d-beb1-1b583d89170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,9).reshape((3,3))\n",
    "y = np.ones((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b88504-9a25-4893-b360-8e0a7de7a6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 2.],\n",
       "       [3., 4., 5.],\n",
       "       [6., 7., 8.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e86fa9-a8d5-4b2e-ad8c-3ffcb9a1452e",
   "metadata": {},
   "source": [
    "# Preprocessing all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52872dd0-c104-4fca-8cf6-0b022649b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = \"../../data/input/sociopattern/\"\n",
    "datasets = os.listdir(in_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc840f94-6194-4bfb-b405-645fe09834f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mworkplace01\u001b[0m\n",
      "initial time contacts: 28820, intial time co_locations: 28820\n",
      "excluded nodes:  set()\n",
      "excluded nodes:  {89, 374, 782}\n",
      "excluded times:  {259.0, 272.0, 19.0, 20.0, 283.0, 284.0, 43.0, 44.0, 56.0, 187.0, 188.0, 68.0, 200.0, 80.0, 212.0, 92.0, 104.0, 236.0, 115.0, 116.0}\n",
      "times in contacts but not in co_locations:  set()\n",
      "T: 109, N: 92\n",
      "Adjacency matrix saved in: ../../data/input/sociopattern/workplace01/data.csv\n",
      "\u001b[1mhighschool\u001b[0m\n",
      "initial time contacts: 1385982020, intial time co_locations: 29960\n",
      "excluded nodes:  set()\n",
      "excluded nodes:  {2}\n",
      "excluded times:  {384992.0, 385025.0, 384993.0, 384994.0, 385097.0, 385001.0, 385073.0, 385049.0, 384991.0}\n",
      "times in contacts but not in co_locations:  set()\n",
      "T: 46, N: 327\n",
      "Adjacency matrix saved in: ../../data/input/sociopattern/highschool/data.csv\n",
      "\u001b[1mhospital\u001b[0m\n",
      "initial time contacts: 140, intial time co_locations: 0\n",
      "excluded nodes:  set()\n",
      "excluded nodes:  {1632, 1513, 1580, 1518, 1590, 1594}\n",
      "excluded times:  {34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 15.0, 61.0}\n",
      "times in contacts but not in co_locations:  {73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0}\n",
      "T: 89, N: 75\n",
      "Adjacency matrix saved in: ../../data/input/sociopattern/hospital/data.csv\n",
      "\u001b[1mprimary_school\u001b[0m\n",
      "initial time contacts: 31220, intial time co_locations: 34240\n",
      "excluded nodes:  set()\n",
      "excluded nodes:  set()\n",
      "excluded times:  {51.0}\n",
      "times in contacts but not in co_locations:  set()\n",
      "T: 106, N: 242\n",
      "Adjacency matrix saved in: ../../data/input/sociopattern/primary_school/data.csv\n",
      "\u001b[1mworkplace02\u001b[0m\n",
      "initial time contacts: 28840, intial time co_locations: 28820\n",
      "excluded nodes:  set()\n",
      "excluded nodes:  {1870, 943}\n",
      "excluded times:  {116.0, 92.0, 236.0}\n",
      "times in contacts but not in co_locations:  set()\n",
      "T: 127, N: 217\n",
      "Adjacency matrix saved in: ../../data/input/sociopattern/workplace02/data.csv\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets: \n",
    "    print_bold(dataset)\n",
    "    contacts = pd.read_csv(in_folder + dataset + '/' + 'contacts.csv', names=[\"time\", \"from\", \"to\"], sep=' ')\n",
    "    labels = pd.read_csv(in_folder + dataset + '/' + 'communities.csv', names=[\"node\", \"community\"], sep='\\t')\n",
    "    co_locations = pd.read_csv(in_folder + dataset + '/' + 'co-locations.csv', names=[\"time\", \"from\", \"to\"], sep=' ')\n",
    "    # print initial times\n",
    "    contacts_initial_time = contacts[\"time\"].head(1).to_numpy()[0]\n",
    "    co_locations_initial_time = co_locations[\"time\"].head(1).to_numpy()[0] \n",
    "    print(f\"initial time contacts: {contacts_initial_time}, intial time co_locations: {co_locations_initial_time}\")\n",
    "    \n",
    "    '''\n",
    "        for the highschool dataset, contacts time is stored in a unix datetime format\n",
    "    '''\n",
    "    if dataset == \"highschool\": \n",
    "        co_locations[\"time\"] = co_locations[\"time\"].map(lambda x: (datetime(year=2013, month=12, day=2) + timedelta(seconds=x)).timestamp())\n",
    "        #contacts[\"time\"] = contacts[\"time\"].map(lambda x: datetime.fromtimestamp(x).time())\n",
    "        #contacts[\"time\"] = contacts[\"time\"].map(lambda t: (t.hour * 60 + t.minute) * 60 + t.second + 3600)\n",
    "        #print(contacts[\"time\"].head(1).to_numpy()[0])\n",
    "        #print(co_locations[\"time\"].head(1).to_numpy()[0])\n",
    "    '''\n",
    "        downsample in time to get a reasonable number of timesteps\n",
    "    '''\n",
    "    contacts, co_locations = downsample_dataset(contacts, co_locations, dataset)\n",
    "    '''\n",
    "    print(\"times in contacts: \", set(contacts[\"time\"]))\n",
    "    print(\"times in co_locations: \", set(co_locations[\"time\"]))\n",
    "    \n",
    "    print(\"nodes in contacts: \", set(contacts[\"from\"]).union(set(contacts[\"to\"])))\n",
    "    print(\"nodes in co_locations: \", set(co_locations[\"from\"]).union(set(co_locations[\"to\"])))\n",
    "    \n",
    "    print(\"nodes in labels: \", set(labels[\"node\"]))\n",
    "    '''\n",
    "    contacts, co_locations, labels = filter_dataset(contacts, co_locations, labels)\n",
    "    contacs, co_locations, labels = relabel_dataset(contacts, co_locations, labels)\n",
    "\n",
    "    \n",
    "    '''\n",
    "        create latent variables and data tensor\n",
    "    '''\n",
    "    T = contacts[\"time\"].unique().size\n",
    "    N = len(labels.index)\n",
    "    K = len(set(labels[\"community\"]))\n",
    "    data = create_data(contacts)\n",
    "    Z_heaviside, Z_markov = create_Z(co_locations, data)\n",
    "    u = np.zeros((N,K))\n",
    "    for i in range(N):\n",
    "        u[i,labels[\"community\"][i]] = 1\n",
    "    v = u.copy()\n",
    "    \n",
    "    '''\n",
    "        store results\n",
    "    '''\n",
    "    write_data(in_folder + dataset + \"/\", \"data\", data)\n",
    "    np.savez(in_folder + dataset + \"/\" + \"params\", u=u, v=v, Z_heaviside=Z_heaviside, Z_markov=Z_markov)\n",
    "    \n",
    "    '''\n",
    "        create and store masks for five-fold cross-validation\n",
    "    '''\n",
    "    mask = extract_mask(T,N)\n",
    "    np.savez(in_folder + dataset + \"/\" + \"mask\", mask=mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d123389-afd5-4152-afab-cc007dc76384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24fbd1-ad9d-4420-9466-4e885a2c7faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a30aa6-81d5-4f37-aea6-6cfe83da6b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
